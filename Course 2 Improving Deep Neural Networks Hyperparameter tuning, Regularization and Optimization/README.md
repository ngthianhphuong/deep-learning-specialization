# Course 2: Improving Deep Neural Networks Hyperparameter Tuning, Regularization and Optimization


## [Programming Assignment 1: Initialization](https://github.com/ngthianhphuong/deep-learning-specialization/tree/master/Course%202%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Initialization)
Skills learned:
- Understand that different initialization methods and their impact on model performance

- Implement zero initialization and and see it fails to "break symmetry"

- Recognize that random initialization "breaks symmetry" and yields more efficient models

- Understand that you could use both random initialization and scaling to get even better training performance on your model.

## [Programming Assignment 2: Regularization](https://github.com/ngthianhphuong/deep-learning-specialization/tree/master/Course%202%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Regularization)
Skills learned:
- Understand different regularization methods that could help your model.

- Implement dropout and see how it works on data.

- Recognize that a model without regularization gives you a better accuracy on the training set but nor necessarily on the test set.

- Understand that you could use both dropout and regularization on your model.

## [Programming Assignment 3: Gradient Checking](https://github.com/ngthianhphuong/deep-learning-specialization/tree/master/Course%202%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Gradient%20Checking)
Skills learned:
- Implement gradient checking from scratch.

- Understand how to use the difference formula to check your backpropagation implementation.

- Recognize that your backpropagation algorithm should give you similar results as the ones you got by computing the difference formula.

- Learn how to identify which parameter's gradient was computed incorrectly.

## [Programming Assignment 4: Gradient Checking](https://github.com/ngthianhphuong/deep-learning-specialization/tree/master/Course%202%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Gradient%20Checking)
Skills learned:
- Implement gradient checking from scratch.

- Understand how to use the difference formula to check your backpropagation implementation.

- Recognize that your backpropagation algorithm should give you similar results as the ones you got by computing the difference formula.

- Learn how to identify which parameter's gradient was computed incorrectly.

## [Programming Assignment 5: Optimization](https://github.com/ngthianhphuong/deep-learning-specialization/tree/master/Course%202%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Optimization)
Skills learned:
- Understand the intuition between Adam and RMS prop

- Recognize the importance of mini-batch gradient descent

- Learn the effects of momentum on the overall performance of your model

## [TensorFlow Programming Assignment](https://github.com/ngthianhphuong/deep-learning-specialization/tree/master/Course%202%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/TensorFlow_Tutorial)
Skills learned:
- Learn all the basics of Tensorflow, implement useful functions, understand what Tensors and operations are, as well as how to execute them in a computation graph.

- Implement your own deep learning models using Tensorflow: using our brand new SIGNS dataset, you will build a deep neural network model to recognize numbers from 0 to 5 in sign language with a pretty impressive accuracy.

